apiVersion: v1
kind: Pod
metadata:
  name: s3-acquire
  namespace: default
  labels:
    name: s3-acquire
    instance: s3-acquire-pod
    version: "1.0.0"
    component: batch
    part-of: pods
spec:
  restartPolicy: Never  # Always
  serviceAccountName: spark
  containers:
    - name: s3-acquire
      image: account-id.dkr.ecr.us-east-2.amazonaws.com/spark-py:3.1.1-3.2
      imagePullPolicy: Always  # IfNotPresent
      args: [
        "/bin/sh",
        "-c",
        "/opt/spark/bin/spark-submit \
        --master k8s://https://[cluster-info].us-east-2.eks.amazonaws.com:443 \
        --deploy-mode cluster \
        --name spark-ingest \
        --conf spark.executor.instances=4 \
        --conf spark.executor.memory=8G \
        --conf spark.executor.cores=2 \
        --conf spark.kubernetes.container.image=[account-id].dkr.ecr.us-east-2.amazonaws.com/spark-ingest:latest \
        --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
        --conf spark.kubernetes.container.image.pullPolicy=Always \ 
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
        --conf spark.kubernetes.executor.deleteOnTermination=false \
        --conf spark.kubernetes.driverEnv.S3_PREFIX=s3a://my_bucket/prefix \
        --conf spark.kubernetes.driverEnv.S3_SUFFIX=suffix_20210428.gz \
        --conf spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=aws-client-secret:AWS_ACCESS_KEY_ID \
        --conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-client-secret:AWS_SECRET_ACCESS_KEY \
        --conf spark.kubernetes.driver.secretKeyRef.TARGET_JDBC_URL=aws-connect-secret:TARGET_JDBC_URL \
        --conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-client-secret:AWS_ACCESS_KEY_ID \
        --conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-client-secret:AWS_SECRET_ACCESS_KEY \
        --conf spark.kubernetes.executor.secretKeyRef.TARGET_JDBC_URL=aws-connect-secret:TARGET_JDBC_URL \
        --conf spark.jars.ivy=/tmp/.ivy \
        local:///opt/spark/spark-ingest/main.py \
        --filepath s3a://bangkok/mnm_dataset.csv"
      ]
      resources:
        requests:
          cpu: "250m"
          memory: "500Mi"
        limits:
          cpu: "4000m"
          memory: "8Gi"